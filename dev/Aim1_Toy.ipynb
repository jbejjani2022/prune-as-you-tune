{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMA86qsAMHtQoHl8wt6mR5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amulyagarimella/242finalproject/blob/main/Aim1_Toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L80x-8fp-jcs"
      },
      "outputs": [],
      "source": [
        "#@title PA2 - unstructured pruning\n",
        "\n",
        "def _make_pair(x):\n",
        "    if hasattr(x, '__len__'):\n",
        "        return x\n",
        "    else:\n",
        "        return (x, x)\n",
        "\n",
        "class SparseConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                     padding=1):\n",
        "        super(SparseConv2d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = _make_pair(stride)\n",
        "        self.padding = _make_pair(padding)\n",
        "\n",
        "        # initialize weights of this layer\n",
        "        self._weight = nn.Parameter(torch.randn([self.out_channels, self.in_channels,\n",
        "                                                        self.kernel_size, self.kernel_size]))\n",
        "        stdv = 1. / math.sqrt(in_channels)\n",
        "        self._weight.data.uniform_(-stdv, stdv)\n",
        "        # initialize mask\n",
        "        # Since we are going to zero out the whole filter, the number of\n",
        "        # elements in the mask is equal to the number of filters.\n",
        "        self.register_buffer('_mask', torch.ones(out_channels))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.conv2d(x, self.weight, stride=self.stride,\n",
        "                        padding=self.padding)\n",
        "    @property\n",
        "    def weight(self):\n",
        "        # check out https://pytorch.org/docs/stable/notes/broadcasting.html\n",
        "        # to better understand the following line\n",
        "        return self._mask[:,None,None,None] * self._weight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code Cell 3.4\n",
        "\n",
        "# unstructurd - remove smallest WEIGHT in each layer\n",
        "# technically train-prune-retrain allows for \"how much of a training you do\"\n",
        "class SparseConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                     padding=1):\n",
        "        super(SparseConv2d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = _make_pair(stride)\n",
        "        self.padding = _make_pair(padding)\n",
        "\n",
        "        # initialize weights of this layer\n",
        "        self._weight = nn.Parameter(torch.randn([self.out_channels, self.in_channels,\n",
        "                                                        self.kernel_size, self.kernel_size]))\n",
        "        stdv = 1. / math.sqrt(in_channels)\n",
        "        self._weight.data.uniform_(-stdv, stdv)\n",
        "        # Since we are going to zero out the whole filter, the number of\n",
        "        # elements in the mask is equal to the number of filters.\n",
        "        self.register_buffer('_mask', torch.ones_like(self._weight))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.conv2d(x, self.weight, stride=self.stride,\n",
        "                        padding=self.padding)\n",
        "\n",
        "    @property\n",
        "    def weight(self):\n",
        "        return self._mask * self._weight\n",
        "\n",
        "\n",
        "def unnstructured_pruning(net, prune_percent):\n",
        "    for i, layer in enumerate(get_sparse_conv2d_layers(net)):\n",
        "        num_nonzero = layer._mask.sum().item()\n",
        "        num_total = layer._mask.numel()\n",
        "        num_prune = round(num_total * prune_percent)\n",
        "        sparsity = 100.0 * (1 - (num_nonzero / num_total))\n",
        "        print(\"Pruning: \", num_prune, num_total, prune_percent)\n",
        "        print(f\"Sparsity before pruning: {sparsity}\")\n",
        "\n",
        "        # We set elements in layer._mask to zero corresponding to the smallest magnitude\n",
        "        abs_weight = torch.abs(layer._weight)\n",
        "        indices = torch.nonzero(layer._mask.view(-1))\n",
        "        # From current nonzero indices in the mask, find those corresponding to lowest-weight filters\n",
        "        # We select from currently-nonzero elements so that we don't redundantly prune\n",
        "        sorted_nonzero_indices = indices[torch.argsort(abs_weight.view(-1)[indices].view(-1))].view(-1)\n",
        "        # Flatten the list of elements, then use unravel_index to get the original indices of the elements we want to prune\n",
        "        idx_to_prune_flat = sorted_nonzero_indices[0:num_prune]\n",
        "        idx_to_prune = torch.unravel_index(idx_to_prune_flat, layer._weight.size())\n",
        "        print(torch.stack(idx_to_prune).shape)\n",
        "        layer._mask.data[idx_to_prune] = 0"
      ],
      "metadata": {
        "id": "8J5-gD0SAReS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PA2 - unstructured pruning - run\n",
        "\n",
        "torch.manual_seed(43) # to give stable randomness\n",
        "\n",
        "def get_sparse_conv2d_layers(net):\n",
        "    '''\n",
        "    Helper function which returns all SparseConv2d layers in the net.\n",
        "    Use this below to implement layerwise pruning.\n",
        "    '''\n",
        "    sparse_conv_layers = []\n",
        "    for layer in net.children():\n",
        "        if isinstance(layer, SparseConv2d):\n",
        "            sparse_conv_layers.append(layer)\n",
        "        else:\n",
        "            child_layers = get_sparse_conv2d_layers(layer)\n",
        "            sparse_conv_layers.extend(child_layers)\n",
        "\n",
        "    return sparse_conv_layers\n",
        "\n",
        "device = 'cuda'\n",
        "net = SparseConvNet()\n",
        "net = net.to(device)\n",
        "\n",
        "# Set these parameters based on PART 1.2\n",
        "lr = 0.1 # best learning rate from 1.2\n",
        "epochs = 100\n",
        "milestones = list(range(25, epochs, 25)) # milestones from 1.2\n",
        "\n",
        "# PART 3.3: Set this prune an additional 10% every 10 epochs, starting at\n",
        "#           epoch 10, ending at epoch 50. By the end, you should achieve\n",
        "#           50% sparsity for each convolution layer in the CNN. Current\n",
        "#           paramaters indicate 10% pruning at the end of epoch 0.\n",
        "prune_percentage = 0.1\n",
        "prune_epochs = np.linspace(10,50,5)\n",
        "\n",
        "\n",
        "def train_and_test_SparseConvNet_unstructured_pruning (lr = 0.1, milestones = [], epochs = 5, prune_percentage = 0, prune_epochs = []):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,\n",
        "                                weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                    milestones=milestones,\n",
        "                                                    gamma=0.1)\n",
        "\n",
        "    train_loss_tracker, train_acc_tracker = [], []\n",
        "    test_loss_tracker, test_acc_tracker = [], []\n",
        "\n",
        "    print('Training for {} epochs, with learning rate {} and milestones {}'.\n",
        "    format(epochs, lr, milestones))\n",
        "\n",
        "    print('Unstructured pruning percentage {} and epochs {}'.format(prune_percentage, prune_epochs))\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(0, epochs):\n",
        "        train(net=net, epoch=epoch, loader=trainloader, criterion=criterion, optimizer=optimizer, loss_tracker=train_loss_tracker, acc_tracker=train_acc_tracker)\n",
        "\n",
        "        if epoch in prune_epochs:\n",
        "            print('\\nUnstructured pruning at epoch {}'.format(epoch))\n",
        "            unnstructured_pruning(net, prune_percentage)\n",
        "            # unstructured_pruning(net, prune_percentage)\n",
        "\n",
        "        test(net=net, epoch=epoch, loader=testloader, criterion=criterion, loss_tracker=test_loss_tracker, acc_tracker=test_acc_tracker)\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print('Total training time: {} seconds'.format(total_time))\n",
        "    return train_loss_tracker, train_acc_tracker, test_loss_tracker, test_acc_tracker"
      ],
      "metadata": {
        "id": "Vn6Abcj_AVEX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}